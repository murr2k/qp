# Testing and Debugging Workflows for QP-QK SDK

## Overview

This guide provides comprehensive testing and debugging workflows specifically designed for AI agents developing microcontroller firmware using the QP framework with QK kernel. It covers unit testing, integration testing, debugging techniques, and validation procedures.

## Testing Strategy

### Test Pyramid for Embedded QK Systems

```
                   ┌─────────────────┐
                   │  System Tests   │  <- Few, expensive, slow
                   │  (Hardware)     │
                   └─────────────────┘
                 ┌─────────────────────┐
                 │ Integration Tests   │  <- Some, moderate cost
                 │ (QUTest + QSpy)     │
                 └─────────────────────┘
               ┌───────────────────────────┐
               │     Unit Tests            │  <- Many, cheap, fast
               │ (Individual AO Testing)   │
               └───────────────────────────┘
```

### Testing Principles for QK Systems

1. **Event-Driven Testing**: Test through event injection rather than direct function calls
2. **State-Based Verification**: Validate state machine behavior and transitions
3. **Timing-Aware Testing**: Verify real-time constraints and deadlines
4. **Non-Blocking Verification**: Ensure all operations complete within RTC bounds
5. **Resource Usage Testing**: Monitor memory pools and queue usage

## Unit Testing Framework

### QUTest Integration

QUTest is the recommended unit testing framework for QP-based systems, providing:
- Target-based testing (tests run on actual hardware)
- Host-based testing (tests run on development machine)
- QS trace-based verification
- Python test scripting

### Unit Test Template for Active Objects

```python
#!/usr/bin/env python3
"""
Unit tests for {{AO_NAME}} Active Object
Generated by QP-QK SDK for AI Agent development
"""

import sys
import os
from qspypy.qspy import *

# Test configuration
TEST_AO = "AO_{{AO_NAME}}"
TIMEOUT_MS = 1000

def setup_test():
    """Setup test environment"""
    # Reset target
    target_reset()
    
    # Initialize QS tracing
    expect("@timestamp BSP_INIT")
    expect("@timestamp QF_init")
    expect("@timestamp %s INIT" % TEST_AO)
    
    # Set up filters
    glb_filter(GRP_ALL)
    current_obj(OBJ_AO, TEST_AO)

def teardown_test():
    """Cleanup after test"""
    # Stop target
    target_reset()

class Test{{AO_NAME}}:
    """Unit tests for {{AO_NAME}} Active Object"""
    
    def test_initial_transition(self):
        """Test initial transition and state entry"""
        setup_test()
        
        # Expect initial transition
        expect("@timestamp ===> Tran Obj=%s,Sig=0,State=0->{{AO_NAME}}_inactive" % TEST_AO)
        expect("@timestamp Entry Obj=%s,State={{AO_NAME}}_inactive" % TEST_AO)
        
        # Verify subscription to published events
        expect("@timestamp Subscribe Obj=%s,Sig=TICK_SIG" % TEST_AO)
        expect("@timestamp Subscribe Obj=%s,Sig=FAULT_SIG" % TEST_AO)
        
        teardown_test()
    
    def test_start_command(self):
        """Test start command handling"""
        setup_test()
        
        # Skip initial transition
        skip_expect("@timestamp ===> Tran")
        skip_expect("@timestamp Entry")
        skip_expect("@timestamp Subscribe", 2)
        
        # Send start command
        post("{{AO_NAME_UPPER}}_START_SIG")
        expect("@timestamp Dispatch Obj=%s,Sig={{AO_NAME_UPPER}}_START_SIG" % TEST_AO)
        expect("@timestamp ===> Tran Obj=%s,Sig={{AO_NAME_UPPER}}_START_SIG,State={{AO_NAME}}_inactive->{{AO_NAME}}_active" % TEST_AO)
        expect("@timestamp Exit Obj=%s,State={{AO_NAME}}_inactive" % TEST_AO)
        expect("@timestamp Entry Obj=%s,State={{AO_NAME}}_active" % TEST_AO)
        
        teardown_test()
    
    def test_periodic_operation(self):
        """Test periodic timer operation"""
        setup_test()
        
        # Transition to active state
        post("{{AO_NAME_UPPER}}_START_SIG")
        skip_expect("@timestamp", 10)  # Skip transition traces
        
        # Verify periodic timer is armed
        expect("@timestamp Timer Arm Obj=%s,Timer=0x*,Nshots=0,Interval=*" % TEST_AO)
        
        # Inject timeout event
        post("{{AO_NAME_UPPER}}_TIMEOUT_SIG")
        expect("@timestamp Dispatch Obj=%s,Sig={{AO_NAME_UPPER}}_TIMEOUT_SIG" % TEST_AO)
        
        # Verify periodic processing
        expect("@timestamp User+000 Obj=%s,Data=*" % TEST_AO)  # Custom trace record
        
        teardown_test()
    
    def test_error_handling(self):
        """Test error condition handling"""
        setup_test()
        
        # Transition to active state
        post("{{AO_NAME_UPPER}}_START_SIG")
        skip_expect("@timestamp", 10)
        
        # Inject error event
        post("{{AO_NAME_UPPER}}_ERROR_SIG")
        expect("@timestamp Dispatch Obj=%s,Sig={{AO_NAME_UPPER}}_ERROR_SIG" % TEST_AO)
        expect("@timestamp ===> Tran Obj=%s,Sig={{AO_NAME_UPPER}}_ERROR_SIG,State=*->{{AO_NAME}}_error" % TEST_AO)
        expect("@timestamp Entry Obj=%s,State={{AO_NAME}}_error" % TEST_AO)
        
        # Verify error counter increment
        expect("@timestamp User+001 Obj=%s,Data=1" % TEST_AO)  # Error count trace
        
        teardown_test()
    
    def test_configuration(self):
        """Test configuration event handling"""
        setup_test()
        
        # Create configuration event
        config_data = {
            'param_id': 1,
            'value': 12345,
            'flags': 0x01
        }
        
        # Post configuration event
        post("{{AO_NAME_UPPER}}_CONFIG_SIG", pack("<HLB", 
             config_data['param_id'], 
             config_data['value'], 
             config_data['flags']))
        
        expect("@timestamp Dispatch Obj=%s,Sig={{AO_NAME_UPPER}}_CONFIG_SIG" % TEST_AO)
        
        # Verify configuration applied
        expect("@timestamp User+002 Obj=%s,Data=%d" % (TEST_AO, config_data['value']))
        
        teardown_test()
    
    def test_timing_constraints(self):
        """Test that all operations complete within RTC bounds"""
        setup_test()
        
        # Enable timing measurements
        glb_filter(GRP_SM)  # State machine group for timing
        
        # Send events and measure processing time
        test_events = [
            "{{AO_NAME_UPPER}}_START_SIG",
            "{{AO_NAME_UPPER}}_CONFIG_SIG",
            "{{AO_NAME_UPPER}}_TIMEOUT_SIG",
            "{{AO_NAME_UPPER}}_STOP_SIG"
        ]
        
        for event in test_events:
            start_time = current_time()
            post(event)
            expect("@timestamp Dispatch")
            end_time = current_time()
            
            processing_time = end_time - start_time
            if processing_time > {{MAX_RTC_TIME_US}}:
                raise AssertionError(f"RTC violation: {event} took {processing_time}us")
        
        teardown_test()
    
    def test_memory_usage(self):
        """Test memory pool usage"""
        setup_test()
        
        # Enable memory pool tracing
        loc_filter(QS_MP_OBJ, 0)  # All memory pools
        
        # Generate events that use memory pools
        for i in range(10):
            post("{{AO_NAME_UPPER}}_CONFIG_SIG")
            expect("@timestamp MP_Get")  # Memory pool get
            expect("@timestamp Dispatch")
            expect("@timestamp MP_Put")  # Memory pool put
        
        # Verify no memory leaks
        expect("@timestamp MP_Put")  # Final cleanup
        
        teardown_test()

# Integration test helpers
def test_inter_ao_communication():
    """Test communication between Active Objects"""
    setup_test()
    
    # Start both AOs
    post("{{AO_NAME_UPPER}}_START_SIG", current_obj(OBJ_AO, TEST_AO))
    post("START_SIG", current_obj(OBJ_AO, "AO_OtherAO"))
    
    # Skip initialization
    skip_expect("@timestamp", 20)
    
    # Test published event
    publish("SENSOR_DATA_SIG", pack("<Hf", 1, 25.5))
    
    # Verify both AOs receive the event
    expect("@timestamp Dispatch Obj=%s,Sig=SENSOR_DATA_SIG" % TEST_AO)
    expect("@timestamp Dispatch Obj=AO_OtherAO,Sig=SENSOR_DATA_SIG")
    
    teardown_test()

def test_priority_preemption():
    """Test QK preemption behavior"""
    setup_test()
    
    # Enable scheduler tracing
    glb_filter(GRP_QK)
    
    # Start low priority AO processing
    current_obj(OBJ_AO, "AO_LowPrio")
    post("LONG_PROCESS_SIG")
    expect("@timestamp Dispatch Obj=AO_LowPrio,Sig=LONG_PROCESS_SIG")
    
    # Inject high priority event
    current_obj(OBJ_AO, TEST_AO)
    post("{{AO_NAME_UPPER}}_START_SIG")
    
    # Verify preemption occurs
    expect("@timestamp QK_sched Obj=AO_LowPrio->%s" % TEST_AO)
    expect("@timestamp Dispatch Obj=%s,Sig={{AO_NAME_UPPER}}_START_SIG" % TEST_AO)
    
    # Verify low priority AO resumes
    expect("@timestamp QK_sched Obj=%s->AO_LowPrio" % TEST_AO)
    
    teardown_test()

# Test runner
if __name__ == "__main__":
    # Initialize QUTest
    qutest_init()
    
    # Run tests
    test_suite = Test{{AO_NAME}}()
    
    test_suite.test_initial_transition()
    test_suite.test_start_command()
    test_suite.test_periodic_operation()
    test_suite.test_error_handling()
    test_suite.test_configuration()
    test_suite.test_timing_constraints()
    test_suite.test_memory_usage()
    
    # Integration tests
    test_inter_ao_communication()
    test_priority_preemption()
    
    print("All tests passed!")
```

## Integration Testing

### System-Level Test Framework

```python
#!/usr/bin/env python3
"""
System integration tests for QP-QK project
Tests complete system behavior with multiple Active Objects
"""

import sys
import time
from qspypy.qspy import *

class SystemIntegrationTests:
    """Complete system integration test suite"""
    
    def __init__(self):
        self.active_objects = [
            "AO_SensorReader",
            "AO_Controller", 
            "AO_Actuator",
            "AO_Display",
            "AO_Communication"
        ]
    
    def test_system_initialization(self):
        """Test complete system startup sequence"""
        target_reset()
        
        # Verify BSP initialization
        expect("@timestamp BSP_INIT")
        
        # Verify QF initialization
        expect("@timestamp QF_init")
        
        # Verify all Active Objects initialize
        for ao in self.active_objects:
            expect("@timestamp %s INIT" % ao)
        
        # Verify system starts
        expect("@timestamp QF_run")
        
    def test_sensor_to_actuator_flow(self):
        """Test complete sensor -> controller -> actuator flow"""
        self.test_system_initialization()
        
        # Skip startup traces
        skip_expect("@timestamp", 50)
        
        # Inject sensor data
        current_obj(OBJ_AO, "AO_SensorReader")
        publish("SENSOR_DATA_SIG", pack("<Hf", 1, 25.5))
        
        # Verify sensor data propagation
        expect("@timestamp Publish Sig=SENSOR_DATA_SIG")
        expect("@timestamp Dispatch Obj=AO_Controller,Sig=SENSOR_DATA_SIG")
        expect("@timestamp Dispatch Obj=AO_Display,Sig=SENSOR_DATA_SIG")
        
        # Verify controller processes data and sends command
        expect("@timestamp Post Obj=AO_Actuator,Sig=ACTUATOR_CMD_SIG")
        expect("@timestamp Dispatch Obj=AO_Actuator,Sig=ACTUATOR_CMD_SIG")
        
        # Verify actuator responds
        expect("@timestamp Post Obj=AO_Controller,Sig=ACTUATOR_RESP_SIG")
        
    def test_fault_tolerance(self):
        """Test system fault tolerance and recovery"""
        self.test_system_initialization()
        skip_expect("@timestamp", 50)
        
        # Inject fault condition
        publish("FAULT_SIG", pack("<HH", 0x1001, 1))  # Major fault
        
        # Verify all AOs receive fault signal
        for ao in self.active_objects:
            expect("@timestamp Dispatch Obj=%s,Sig=FAULT_SIG" % ao)
        
        # Verify system enters safe mode
        expect("@timestamp ===> Tran Obj=AO_Controller,State=*->safe_mode")
        
        # Verify non-critical systems shut down
        expect("@timestamp Exit Obj=AO_Display,State=active")
        expect("@timestamp Exit Obj=AO_Communication,State=active")
        
        # Test recovery
        publish("RECOVERY_SIG")
        expect("@timestamp ===> Tran Obj=AO_Controller,State=safe_mode->operational")
        
    def test_timing_and_scheduling(self):
        """Test real-time scheduling behavior"""
        self.test_system_initialization()
        skip_expect("@timestamp", 50)
        
        # Enable QK scheduler tracing
        glb_filter(GRP_QK)
        
        # Generate mixed priority events
        current_obj(OBJ_AO, "AO_SensorReader")  # High priority
        post("READ_SENSOR_SIG")
        
        current_obj(OBJ_AO, "AO_Communication")  # Low priority
        post("COMM_DATA_SIG")
        
        current_obj(OBJ_AO, "AO_Controller")     # Medium priority
        post("CONTROL_SIG")
        
        # Verify scheduling order (highest priority first)
        expect("@timestamp QK_sched")
        expect("@timestamp Dispatch Obj=AO_SensorReader,Sig=READ_SENSOR_SIG")
        expect("@timestamp Dispatch Obj=AO_Controller,Sig=CONTROL_SIG")
        expect("@timestamp Dispatch Obj=AO_Communication,Sig=COMM_DATA_SIG")
        
    def test_memory_pool_management(self):
        """Test system-wide memory pool usage"""
        self.test_system_initialization()
        skip_expect("@timestamp", 50)
        
        # Enable memory pool tracing
        loc_filter(QS_MP_OBJ, 0)
        
        # Generate events that stress memory pools
        for i in range(20):
            publish("SENSOR_DATA_SIG", pack("<Hf", i, float(i * 1.5)))
            
            # Verify pool allocation/deallocation
            expect("@timestamp MP_Get")
            expect("@timestamp Publish")
            expect("@timestamp MP_Put")
        
        # Verify no pool exhaustion
        # Pool usage should return to initial state
        
    def test_performance_metrics(self):
        """Measure system performance metrics"""
        self.test_system_initialization()
        skip_expect("@timestamp", 50)
        
        # Measure event processing latency
        start_time = current_time()
        publish("SENSOR_DATA_SIG", pack("<Hf", 1, 25.0))
        
        # Wait for complete processing chain
        expect("@timestamp Post Obj=AO_Actuator,Sig=ACTUATOR_CMD_SIG")
        end_time = current_time()
        
        latency = end_time - start_time
        print(f"End-to-end latency: {latency} us")
        
        # Verify latency meets requirements
        if latency > 1000:  # 1ms max latency
            raise AssertionError(f"Latency requirement violated: {latency}us")
        
        # Measure CPU utilization
        # Run system for 1 second and measure idle time
        idle_count = 0
        total_count = 0
        
        start_time = current_time()
        while (current_time() - start_time) < 1000000:  # 1 second
            if check_idle():
                idle_count += 1
            total_count += 1
        
        cpu_utilization = (1.0 - float(idle_count) / total_count) * 100
        print(f"CPU utilization: {cpu_utilization:.1f}%")

def run_all_tests():
    """Run complete integration test suite"""
    suite = SystemIntegrationTests()
    
    tests = [
        suite.test_system_initialization,
        suite.test_sensor_to_actuator_flow,
        suite.test_fault_tolerance,
        suite.test_timing_and_scheduling,
        suite.test_memory_pool_management,
        suite.test_performance_metrics
    ]
    
    for test in tests:
        print(f"Running {test.__name__}...")
        try:
            test()
            print("PASSED")
        except Exception as e:
            print(f"FAILED: {e}")
            return False
    
    print("All integration tests passed!")
    return True

if __name__ == "__main__":
    qutest_init()
    success = run_all_tests()
    sys.exit(0 if success else 1)
```

## Debugging Workflows

### QS/QSpy Debugging Setup

```c
// Debug configuration in main.c
#ifdef Q_SPY

// QS buffer configuration for debugging
#define QS_TX_BUFFER_SIZE   8192U   // Large buffer for debug traces
#define QS_RX_BUFFER_SIZE   512U    // Command buffer

static uint8_t l_qsTxBuf[QS_TX_BUFFER_SIZE];
static uint8_t l_qsRxBuf[QS_RX_BUFFER_SIZE];

void debug_setup(void) {
    // Initialize QS with large buffers
    QS_INIT(l_qsTxBuf, sizeof(l_qsTxBuf),
            l_qsRxBuf, sizeof(l_qsRxBuf));
    
    // Enable all trace records for debugging
    QS_GLB_FILTER(QS_ALL_RECORDS);
    
    // Disable high-frequency records to reduce noise
    QS_GLB_FILTER(-QS_QF_TICK);
    QS_GLB_FILTER(-QS_QF_INT_DISABLE);
    QS_GLB_FILTER(-QS_QF_INT_ENABLE);
    
    // Enable specific AO tracing
    QS_LOC_FILTER(QS_AO_OBJ, &AO_SensorReader);
    QS_LOC_FILTER(QS_AO_OBJ, &AO_Controller);
    QS_LOC_FILTER(QS_AO_OBJ, &AO_Actuator);
    
    // Set up dictionaries for readable output
    setup_qs_dictionaries();
}

void setup_qs_dictionaries(void) {
    // Signal dictionary
    QS_SIG_DICTIONARY(SENSOR_DATA_SIG, (void*)0);
    QS_SIG_DICTIONARY(ACTUATOR_CMD_SIG, (void*)0);
    QS_SIG_DICTIONARY(FAULT_SIG, (void*)0);
    
    // Object dictionary
    QS_OBJ_DICTIONARY(&AO_SensorReader);
    QS_OBJ_DICTIONARY(&AO_Controller);
    QS_OBJ_DICTIONARY(&AO_Actuator);
    
    // Function dictionary
    QS_FUN_DICTIONARY(&SensorReader_initial);
    QS_FUN_DICTIONARY(&SensorReader_active);
    QS_FUN_DICTIONARY(&Controller_processing);
    
    // User record dictionary
    QS_USR_DICTIONARY(SENSOR_READING);
    QS_USR_DICTIONARY(CONTROL_OUTPUT);
    QS_USR_DICTIONARY(ERROR_DETECTED);
}

#endif // Q_SPY
```

### Advanced Debugging Techniques

#### 1. State Machine Debugging

```c
// Add state entry/exit tracing
QState MyAO_active(MyAO * const me, QEvt const * const e) {
    switch (e->sig) {
        case Q_ENTRY_SIG: {
            QS_BEGIN_ID(QS_QEP_STATE_ENTRY, QS_AO_OBJ)
                QS_OBJ_(me);
                QS_FUN_(&MyAO_active);
            QS_END_()
            
            // Entry actions...
            return Q_HANDLED();
        }
        
        case Q_EXIT_SIG: {
            QS_BEGIN_ID(QS_QEP_STATE_EXIT, QS_AO_OBJ)
                QS_OBJ_(me);
                QS_FUN_(&MyAO_active);
            QS_END_()
            
            // Exit actions...
            return Q_HANDLED();
        }
        
        case MY_EVENT_SIG: {
            QS_BEGIN_ID(MY_EVENT_TRACE, QS_AO_OBJ)
                QS_OBJ_(me);
                QS_SIG_(e->sig);
                QS_U32_(me->counter);
            QS_END_()
            
            // Event processing...
            return Q_TRAN(&MyAO_next_state);
        }
    }
    return Q_SUPER(&QHsm_top);
}
```

#### 2. Performance Debugging

```c
// Measure event processing time
QState MyAO_performance_debug(MyAO * const me, QEvt const * const e) {
    uint32_t start_time = BSP_getTime();
    
    QState result = MyAO_normal_handler(me, e);
    
    uint32_t end_time = BSP_getTime();
    uint32_t processing_time = end_time - start_time;
    
    // Log if processing time exceeds threshold
    if (processing_time > MAX_RTC_TIME_US) {
        QS_BEGIN_ID(RTC_VIOLATION, QS_AO_OBJ)
            QS_OBJ_(me);
            QS_SIG_(e->sig);
            QS_U32_(processing_time);
        QS_END_()
    }
    
    return result;
}

// Memory usage debugging
void debug_memory_usage(void) {
    static uint32_t last_check = 0;
    uint32_t current_time = BSP_getTime();
    
    // Check every 1000ms
    if ((current_time - last_check) > 1000000) {
        // Check event pool usage
        uint16_t small_pool_free = QF_getPoolMin(sizeof(SmallEvt));
        uint16_t med_pool_free = QF_getPoolMin(sizeof(MediumEvt));
        uint16_t large_pool_free = QF_getPoolMin(sizeof(LargeEvt));
        
        QS_BEGIN_ID(MEMORY_USAGE, 0U)
            QS_U16_(small_pool_free);
            QS_U16_(med_pool_free);
            QS_U16_(large_pool_free);
            QS_TIME_();
        QS_END_()
        
        last_check = current_time;
    }
}
```

#### 3. Real-Time Debugging

```python
#!/usr/bin/env python3
"""
Real-time debugging script using QSpy
Analyzes system behavior in real-time
"""

import sys
from qspypy.qspy import *

class RealTimeDebugger:
    def __init__(self):
        self.event_counts = {}
        self.ao_states = {}
        self.timing_violations = []
        self.memory_warnings = []
    
    def process_trace_record(self, record):
        """Process each trace record in real-time"""
        
        if record.rec_type == "DISPATCH":
            # Count events per AO
            ao_name = record.obj_name
            event_sig = record.sig_name
            
            if ao_name not in self.event_counts:
                self.event_counts[ao_name] = {}
            
            if event_sig not in self.event_counts[ao_name]:
                self.event_counts[ao_name][event_sig] = 0
            
            self.event_counts[ao_name][event_sig] += 1
            
        elif record.rec_type == "STATE_TRAN":
            # Track state transitions
            ao_name = record.obj_name
            target_state = record.target_state
            self.ao_states[ao_name] = target_state
            
        elif record.rec_type == "RTC_VIOLATION":
            # Log timing violations
            violation = {
                'timestamp': record.timestamp,
                'ao_name': record.obj_name,
                'event_sig': record.sig_name,
                'processing_time': record.data
            }
            self.timing_violations.append(violation)
            
            print(f"⚠️  RTC VIOLATION: {violation}")
            
        elif record.rec_type == "MEMORY_USAGE":
            # Monitor memory usage
            small_free = record.data[0]
            med_free = record.data[1]
            large_free = record.data[2]
            
            if small_free < 5:
                warning = {
                    'timestamp': record.timestamp,
                    'pool': 'small',
                    'free_count': small_free
                }
                self.memory_warnings.append(warning)
                print(f"⚠️  MEMORY WARNING: {warning}")
    
    def print_statistics(self):
        """Print debugging statistics"""
        print("\n=== DEBUGGING STATISTICS ===")
        
        print("\nEvent Counts:")
        for ao_name, events in self.event_counts.items():
            print(f"  {ao_name}:")
            for event_sig, count in events.items():
                print(f"    {event_sig}: {count}")
        
        print("\nCurrent States:")
        for ao_name, state in self.ao_states.items():
            print(f"  {ao_name}: {state}")
        
        print(f"\nTiming Violations: {len(self.timing_violations)}")
        print(f"Memory Warnings: {len(self.memory_warnings)}")

def main():
    """Main debugging loop"""
    debugger = RealTimeDebugger()
    
    # Connect to QSpy
    qspy_connect()
    
    try:
        while True:
            record = qspy_receive()
            if record:
                debugger.process_trace_record(record)
            
            # Print statistics every 10 seconds
            if time.time() % 10 == 0:
                debugger.print_statistics()
                
    except KeyboardInterrupt:
        print("\nDebugging session ended")
        debugger.print_statistics()

if __name__ == "__main__":
    main()
```

## Automated Testing Pipeline

### Continuous Integration Test Script

```bash
#!/bin/bash
# QP-QK SDK Automated Test Pipeline

PROJECT_DIR=$1
if [ -z "$PROJECT_DIR" ]; then
    echo "Usage: $0 <project_directory>"
    exit 1
fi

cd "$PROJECT_DIR"

echo "🚀 Starting QP-QK SDK Test Pipeline"
echo "Project: $(basename $PWD)"
echo "Timestamp: $(date)"
echo

# Step 1: Build the project
echo "📦 Building project..."
python3 ../QP-QK-SDK/tools/builders/build.py --project . --config release
if [ $? -ne 0 ]; then
    echo "❌ Build failed"
    exit 1
fi
echo "✅ Build successful"

# Step 2: Run unit tests
echo "🧪 Running unit tests..."
if [ -d "tests/unit" ]; then
    for test_file in tests/unit/*.py; do
        echo "  Running $(basename $test_file)..."
        python3 "$test_file"
        if [ $? -ne 0 ]; then
            echo "❌ Unit test failed: $(basename $test_file)"
            exit 1
        fi
    done
    echo "✅ All unit tests passed"
else
    echo "⚠️  No unit tests found"
fi

# Step 3: Run integration tests
echo "🔧 Running integration tests..."
if [ -d "tests/integration" ]; then
    for test_file in tests/integration/*.py; do
        echo "  Running $(basename $test_file)..."
        python3 "$test_file"
        if [ $? -ne 0 ]; then
            echo "❌ Integration test failed: $(basename $test_file)"
            exit 1
        fi
    done
    echo "✅ All integration tests passed"
else
    echo "⚠️  No integration tests found"
fi

# Step 4: Static analysis
echo "🔍 Running static analysis..."
if command -v cppcheck &> /dev/null; then
    cppcheck --enable=all --error-exitcode=1 src/
    if [ $? -ne 0 ]; then
        echo "❌ Static analysis failed"
        exit 1
    fi
    echo "✅ Static analysis passed"
else
    echo "⚠️  cppcheck not available, skipping static analysis"
fi

# Step 5: Memory analysis
echo "💾 Analyzing memory usage..."
if [ -f "build/firmware.elf" ]; then
    arm-none-eabi-size -A build/firmware.elf > build/memory_report.txt
    
    # Check for memory overflow
    FLASH_USAGE=$(arm-none-eabi-size build/firmware.elf | tail -1 | awk '{print $1}')
    RAM_USAGE=$(arm-none-eabi-size build/firmware.elf | tail -1 | awk '{print $2+$3}')
    
    # Platform-specific limits (can be configured)
    FLASH_LIMIT=524288  # 512KB
    RAM_LIMIT=131072    # 128KB
    
    if [ $FLASH_USAGE -gt $FLASH_LIMIT ]; then
        echo "❌ Flash usage ($FLASH_USAGE bytes) exceeds limit ($FLASH_LIMIT bytes)"
        exit 1
    fi
    
    if [ $RAM_USAGE -gt $RAM_LIMIT ]; then
        echo "❌ RAM usage ($RAM_USAGE bytes) exceeds limit ($RAM_LIMIT bytes)"
        exit 1
    fi
    
    echo "✅ Memory usage within limits"
    echo "   Flash: $FLASH_USAGE / $FLASH_LIMIT bytes"
    echo "   RAM: $RAM_USAGE / $RAM_LIMIT bytes"
fi

# Step 6: Generate test report
echo "📊 Generating test report..."
cat > build/test_report.md << EOF
# Test Report

**Project:** $(basename $PWD)
**Date:** $(date)
**Status:** ✅ PASSED

## Build Information
- Configuration: Release
- Platform: $(grep platform build_config.yaml | cut -d: -f2 | tr -d ' ')
- Toolchain: $(grep toolchain build_config.yaml | cut -d: -f2 | tr -d ' ')

## Memory Usage
- Flash: $FLASH_USAGE / $FLASH_LIMIT bytes ($(($FLASH_USAGE * 100 / $FLASH_LIMIT))%)
- RAM: $RAM_USAGE / $RAM_LIMIT bytes ($(($RAM_USAGE * 100 / $RAM_LIMIT))%)

## Test Results
- Unit Tests: ✅ PASSED
- Integration Tests: ✅ PASSED
- Static Analysis: ✅ PASSED
- Memory Analysis: ✅ PASSED

## Files Generated
- Firmware: build/firmware.bin
- Debug Info: build/firmware.elf
- Memory Report: build/memory_report.txt
- Test Report: build/test_report.md
EOF

echo "✅ Test pipeline completed successfully!"
echo "📄 Test report generated: build/test_report.md"
```

## Validation and Quality Assurance

### AI Agent Code Validation Checklist

```python
#!/usr/bin/env python3
"""
QP-QK SDK Code Validation Tool
Validates generated code for compliance with QK patterns
"""

import os
import re
import ast
from pathlib import Path

class QKCodeValidator:
    """Validates QP-QK code for common issues"""
    
    def __init__(self):
        self.violations = []
        self.warnings = []
        
    def validate_file(self, file_path: Path):
        """Validate a single C file"""
        with open(file_path, 'r') as f:
            content = f.read()
        
        self.check_blocking_calls(content, file_path)
        self.check_shared_state(content, file_path)
        self.check_dynamic_memory(content, file_path)
        self.check_rte_compliance(content, file_path)
        self.check_qk_patterns(content, file_path)
    
    def check_blocking_calls(self, content: str, file_path: Path):
        """Check for blocking function calls"""
        blocking_patterns = [
            r'HAL_Delay\s*\(',
            r'while\s*\([^)]*\)\s*{[^}]*}',
            r'for\s*\([^)]*;\s*[^;]*;\s*[^)]*\)\s*{[^}]*}',
            r'scanf\s*\(',
            r'gets\s*\(',
            r'sleep\s*\(',
            r'usleep\s*\('
        ]
        
        for pattern in blocking_patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                line_num = content[:match.start()].count('\n') + 1
                self.violations.append({
                    'file': file_path,
                    'line': line_num,
                    'type': 'blocking_call',
                    'message': f'Blocking call detected: {match.group()}'
                })
    
    def check_shared_state(self, content: str, file_path: Path):
        """Check for shared mutable state"""
        # Look for global variables (not const)
        pattern = r'^\s*(?!const\s)[a-zA-Z_][a-zA-Z0-9_]*\s+[a-zA-Z_][a-zA-Z0-9_]*\s*[=;]'
        
        lines = content.split('\n')
        for i, line in enumerate(lines):
            if re.match(pattern, line) and 'static' not in line:
                self.warnings.append({
                    'file': file_path,
                    'line': i + 1,
                    'type': 'shared_state',
                    'message': f'Possible shared mutable state: {line.strip()}'
                })
    
    def check_dynamic_memory(self, content: str, file_path: Path):
        """Check for dynamic memory allocation"""
        dynamic_patterns = [
            r'malloc\s*\(',
            r'calloc\s*\(',
            r'realloc\s*\(',
            r'free\s*\(',
            r'new\s+',
            r'delete\s+'
        ]
        
        for pattern in dynamic_patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                line_num = content[:match.start()].count('\n') + 1
                self.violations.append({
                    'file': file_path,
                    'line': line_num,
                    'type': 'dynamic_memory',
                    'message': f'Dynamic memory allocation detected: {match.group()}'
                })
    
    def check_rte_compliance(self, content: str, file_path: Path):
        """Check Run-to-Completion compliance"""
        # Look for potentially long loops
        long_loop_pattern = r'for\s*\([^)]*;\s*[^<>!=]*[<>]=?\s*\d{4,}'
        
        matches = re.finditer(long_loop_pattern, content)
        for match in matches:
            line_num = content[:match.start()].count('\n') + 1
            self.warnings.append({
                'file': file_path,
                'line': line_num,
                'type': 'long_rte',
                'message': f'Potentially long RTC detected: {match.group()}'
            })
    
    def check_qk_patterns(self, content: str, file_path: Path):
        """Check for proper QK pattern usage"""
        # Check for proper QK ISR entry/exit
        if 'IRQHandler' in content:
            if 'QK_ISR_ENTRY()' not in content:
                self.violations.append({
                    'file': file_path,
                    'line': 0,
                    'type': 'qk_isr',
                    'message': 'IRQ handler missing QK_ISR_ENTRY()'
                })
                
            if 'QK_ISR_EXIT()' not in content:
                self.violations.append({
                    'file': file_path,
                    'line': 0,
                    'type': 'qk_isr',
                    'message': 'IRQ handler missing QK_ISR_EXIT()'
                })
        
        # Check for proper event posting
        post_pattern = r'QACTIVE_POST\s*\([^)]*\)'
        if re.search(post_pattern, content):
            # Should use proper sender parameter
            improper_post = r'QACTIVE_POST\s*\([^,]*,\s*[^,]*\s*\)'
            if re.search(improper_post, content):
                matches = re.finditer(improper_post, content)
                for match in matches:
                    line_num = content[:match.start()].count('\n') + 1
                    self.warnings.append({
                        'file': file_path,
                        'line': line_num,
                        'type': 'event_post',
                        'message': 'QACTIVE_POST missing sender parameter'
                    })
    
    def validate_project(self, project_path: Path):
        """Validate entire project"""
        c_files = list(project_path.glob('**/*.c'))
        h_files = list(project_path.glob('**/*.h'))
        
        for file_path in c_files + h_files:
            self.validate_file(file_path)
    
    def print_report(self):
        """Print validation report"""
        print("🔍 QP-QK Code Validation Report")
        print("=" * 40)
        
        if self.violations:
            print(f"\n❌ VIOLATIONS ({len(self.violations)}):")
            for violation in self.violations:
                print(f"  {violation['file']}:{violation['line']} - {violation['message']}")
        
        if self.warnings:
            print(f"\n⚠️  WARNINGS ({len(self.warnings)}):")
            for warning in self.warnings:
                print(f"  {warning['file']}:{warning['line']} - {warning['message']}")
        
        if not self.violations and not self.warnings:
            print("\n✅ No issues found!")
        
        return len(self.violations) == 0

def main():
    import sys
    
    if len(sys.argv) != 2:
        print("Usage: python validate.py <project_path>")
        sys.exit(1)
    
    project_path = Path(sys.argv[1])
    validator = QKCodeValidator()
    
    validator.validate_project(project_path)
    success = validator.print_report()
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
```

This comprehensive testing and debugging guide provides AI agents with the tools and workflows needed to develop, test, and validate robust QP-QK firmware. The combination of unit testing, integration testing, real-time debugging, and automated validation ensures high-quality embedded software development.